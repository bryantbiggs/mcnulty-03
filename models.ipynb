{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pickled Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (7326,)\n",
      "Testing labels shape:  (2603,)\n"
     ]
    }
   ],
   "source": [
    "directory = 'generated_data'\n",
    "\n",
    "train_labels = np.load('{0}/train_labels.dat'.format(directory))\n",
    "test_labels = np.load('{0}/test_labels.dat'.format(directory))\n",
    "print('Training labels shape: {0}'.format(train_labels.shape))\n",
    "print('Testing labels shape:  {0}'.format(test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Custom Conversion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Conversion training shape: (7326, 640)\n",
      "Custom Conversion testing shape:  (2603, 640)\n"
     ]
    }
   ],
   "source": [
    "train_CustGray_2d = np.load('{0}/train_CustGray_2d.dat'.format(directory))\n",
    "test_CustGray_2d = np.load('{0}/test_CustGray_2d.dat'.format(directory))\n",
    "print('Custom Conversion training shape: {0}'.format(train_CustGray_2d.shape))\n",
    "print('Custom Conversion testing shape:  {0}'.format(test_CustGray_2d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Otsu's Binarization Threshold Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otsu training shape: (7326, 640)\n",
      "Otsu testing shape:  (2603, 640)\n"
     ]
    }
   ],
   "source": [
    "train_OBT_2d = np.load('{0}/train_OBT_2d.dat'.format(directory))\n",
    "test_OBT_2d = np.load('{0}/test_OBT_2d.dat'.format(directory))\n",
    "print('Otsu training shape: {0}'.format(train_OBT_2d.shape))\n",
    "print('Otsu testing shape:  {0}'.format(test_OBT_2d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Adaptive Mean Threshold Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive mean training shape: (7326, 640)\n",
      "Adaptive mean testing shape:  (2603, 640)\n"
     ]
    }
   ],
   "source": [
    "train_AMT_2d = np.load('{0}/train_AMT_2d.dat'.format(directory))\n",
    "test_AMT_2d = np.load('{0}/test_AMT_2d.dat'.format(directory))\n",
    "print('Adaptive mean training shape: {0}'.format(train_AMT_2d.shape))\n",
    "print('Adaptive mean testing shape:  {0}'.format(test_AMT_2d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Adaptive Gaussian Threshold Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive Gaussian training shape: (7326, 640)\n",
      "Adaptive Gaussian testing shape:  (2603, 640)\n"
     ]
    }
   ],
   "source": [
    "train_AGT_2d = np.load('{0}/train_AGT_2d.dat'.format(directory))\n",
    "test_AGT_2d = np.load('{0}/test_AGT_2d.dat'.format(directory))\n",
    "print('Adaptive Gaussian training shape: {0}'.format(train_AGT_2d.shape))\n",
    "print('Adaptive Gaussian testing shape:  {0}'.format(test_AGT_2d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Principle Component Analysis Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA training shape: (7326, 40)\n",
      "PCA testing shape:  (2603, 40)\n"
     ]
    }
   ],
   "source": [
    "train_PCA_2d = np.load('{0}/train_PCA_2d.dat'.format(directory))\n",
    "test_PCA_2d = np.load('{0}/test_PCA_2d.dat'.format(directory))\n",
    "print('PCA training shape: {0}'.format(train_PCA_2d.shape))\n",
    "print('PCA testing shape:  {0}'.format(test_PCA_2d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lists of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ['Cust', 'OBT', 'AMT', 'AGT', 'PCA']\n",
    "l_train = [train_CustGray_2d, train_OBT_2d, train_AMT_2d, train_AGT_2d, train_PCA_2d]\n",
    "l_test = [test_CustGray_2d, test_OBT_2d, test_AMT_2d, test_AGT_2d, test_PCA_2d]\n",
    "overall_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Important Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def important_pixels(mdl, ht, wd, save_f):\n",
    "    '''\n",
    "    source: http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html#example-ensemble-plot-forest-importances-faces-py\n",
    "    '''\n",
    "    importances = mdl.feature_importances_\n",
    "    importances = importances.reshape(ht,wd)\n",
    "\n",
    "    # Plot pixel importances\n",
    "    plt.matshow(importances, cmap=plt.cm.hot)\n",
    "    plt.savefig(save_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Attributes\n",
    "# estimators_ : list of classifiers\n",
    "# classes_ : array of shape = [n_classes]\n",
    "# n_classes_ : int\n",
    "# estimator_weights_ : array of floats\n",
    "# estimator_errors_ : array of floats\n",
    "# feature_importances_ : array of shape = [n_features]\n",
    "\n",
    "ab_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    n_estimators=50\n",
    "    ab_clf = AdaBoostClassifier(base_estimator=None, n_estimators=n_estimators, learning_rate=1.0, \n",
    "                                 algorithm='SAMME.R', random_state=None)\n",
    "    scores = cross_val_score(ab_clf, dataset, train_labels)\n",
    "    ab_dict[('ada_boost_{0}'.format(names[i]),n_estimators)] = scores.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('ada_boost_AGT', 50): 19.601571674291097,\n",
       " ('ada_boost_AMT', 50): 19.901917264897509,\n",
       " ('ada_boost_Cust', 50): 20.760998576551458,\n",
       " ('ada_boost_OBT', 50): 20.48886398197746,\n",
       " ('ada_boost_PCA', 50): 28.022917438729621}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_dict['ada_boost'] = ab_dict\n",
    "ab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Attributes\n",
    "# base_estimator_ : list of estimators\n",
    "# estimators_ : list of estimators\n",
    "# estimators_samples_ : list of arrays\n",
    "# estimators_features_ : list of arrays\n",
    "# classes_ : array of shape = [n_classes]\n",
    "# n_classes_ : int or list\n",
    "# oob_score_ : float\n",
    "# oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
    "\n",
    "bag_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    bag_clf = BaggingClassifier(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, \n",
    "                                bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, \n",
    "                                n_jobs=-1, random_state=None, verbose=0)\n",
    "    scores = cross_val_score(bag_clf, dataset, train_labels)\n",
    "    bag_dict['bagging_{0}'.format(names[i])] = scores.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_AGT': 51.323460316333801,\n",
       " 'bagging_AMT': 56.659094020191304,\n",
       " 'bagging_Cust': 45.304261573046027,\n",
       " 'bagging_OBT': 59.773823683722483,\n",
       " 'bagging_PCA': 45.835940750864538}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_dict['bagging'] = bag_dict\n",
    "bag_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier - TAKES A LONG TIME TO RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Attributes\n",
    "# feature_importances_ : array, shape = [n_features]\n",
    "# oob_improvement_ : array, shape = [n_estimators]\n",
    "# train_score_ : array, shape = [n_estimators]\n",
    "# loss_ : LossFunction\n",
    "# init : BaseEstimator\n",
    "# estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, loss_.K]\n",
    "    \n",
    "gb_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    n_estimators=100\n",
    "    gb_clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=n_estimators, subsample=1.0, \n",
    "                                     min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                     max_depth=3, init=None, random_state=None, max_features=None, verbose=0, \n",
    "                                     max_leaf_nodes=None, warm_start=False, presort='auto')\n",
    "    scores = cross_val_score(gb_clf, dataset, train_labels)\n",
    "    gb_dict[('gradient_boost_{0}'.format(names[i]),n_estimators)] = scores.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_dict['gradient_boost'] = gb_dict\n",
    "gb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# classes_ : array of shape = [n_classes] or a list of such arrays\n",
    "# feature_importances_ : array of shape = [n_features]\n",
    "# max_features_ : int,\n",
    "# n_classes_ : int or list\n",
    "# n_features_ : int\n",
    "# n_outputs_ : int\n",
    "# tree_ : Tree object\n",
    "\n",
    "dtr_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    dtr_clf = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, \n",
    "                            min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
    "                            max_leaf_nodes=None, class_weight=None, presort=False)\n",
    "\n",
    "    scores = cross_val_score(dtr_clf, dataset, train_labels)\n",
    "    dtr_dict['decision_tree_{0}'.format(names[i])] = scores.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision_tree_AGT': 36.144703900143284,\n",
       " 'decision_tree_AMT': 40.171028268996253,\n",
       " 'decision_tree_Cust': 32.570295247530275,\n",
       " 'decision_tree_OBT': 45.412963166240502,\n",
       " 'decision_tree_PCA': 33.170064779635595}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_dict['decision_tree'] = dtr_dict\n",
    "dtr_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etr_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    \n",
    "    etr_clf = ExtraTreesClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                    min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "                    bootstrap=False, oob_score=False, n_jobs=-1, random_state=None, verbose=0, warm_start=False, \n",
    "                    class_weight=None)\n",
    "\n",
    "    scores = cross_val_score(etr_clf, dataset, train_labels)\n",
    "    etr_dict['extra_trees_{0}'.format(names[i])] = scores.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extra_trees_AGT': 51.555399075043461,\n",
       " 'extra_trees_AMT': 57.69721530246126,\n",
       " 'extra_trees_Cust': 43.993940734755469,\n",
       " 'extra_trees_OBT': 60.046629152872391,\n",
       " 'extra_trees_PCA': 38.710884891591277}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_dict['extra_trees'] = etr_dict\n",
    "etr_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Attributes\n",
    "# estimators_ : list of DecisionTreeClassifier\n",
    "# classes_ : array of shape = [n_classes] or a list of such arrays\n",
    "# n_classes_ : int or list\n",
    "# n_features_ : int\n",
    "# n_outputs_ : int\n",
    "# feature_importances_ : array of shape = [n_features]\n",
    "# oob_score_ : float\n",
    "# oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
    "    \n",
    "rf_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    rf_clf = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                                min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                                max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=-1, random_state=None, \n",
    "                                verbose=0, warm_start=False, class_weight=None)\n",
    "    \n",
    "    scores = cross_val_score(rf_clf, dataset, train_labels)\n",
    "    rf_dict['random_forrest_{0}'.format(names[i])] = scores.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_forrest_AGT': 48.169410711487778,\n",
       " 'random_forrest_AMT': 54.16189218059845,\n",
       " 'random_forrest_Cust': 42.478717979674528,\n",
       " 'random_forrest_OBT': 56.975636870828282,\n",
       " 'random_forrest_PCA': 42.274185093343}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_dict['random_forrest'] = rf_dict\n",
    "rf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_dict = {}\n",
    "for i in range(1,20):\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_mdl = knn_clf.fit(train_AMT_2d, train_labels)\n",
    "    knn_dict[i] = '{0:.2f}%'.format(knn_mdl.score(test_AMT_2d, test_labels)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(knn_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Training score is:  {0:.2f}%'.format(knn_mdl.score(train_AMT_2d, train_labels)*100))\n",
    "print('Test score is: {0:.2f}%'.format(knn_mdl.score(test_AMT_2d, test_labels)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = '''\n",
    "svc_clf = SVC()\n",
    "svc_mdl = svc_clf.fit(train_AMT_2d, train_labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = np.array([1., 10., 100., 1000.])\n",
    "gamma = np.array([0.0001, 0.001, 0.01, 0.1])\n",
    "\n",
    "#class sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False,\n",
    "#            tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=None, \n",
    "\n",
    "svd_dict = {}\n",
    "\n",
    "for x in range(len(c)):\n",
    "    for y in range(len(gamma)):\n",
    "        svc_clf = SVC()\n",
    "        svc_mdl = svc_clf.fit(train_CustGray_2d, train_labels)\n",
    "        svd_dict[(c[x],gamma[y])] = svc_mdl.score(test_CustGray_2d, test_labels)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(svd_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Training score is:  {0}%'.format(svc_mdl.score(train_AMT_2d, train_labels)*100))\n",
    "print('Test score is: {0}%'.format(svc_mdl.score(test_AMT_2d, test_labels)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_parameters = svc_grid.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print('\\t{0}: {1}'.format(param_name, best_parameters[param_name]))\n",
    "    \n",
    "print('Training score is', svc_mdl.score(train_data, train_labels))\n",
    "print('Test score is', svc_mdl.score(test_data, test_labels))\n",
    "print('-'*80)\n",
    "print('Classification report of training data:\\n', classification_report(train_labels, svc_mdl.predict(train_data)))\n",
    "print('-'*80)\n",
    "print('Classification report of test data:\\n', classification_report(test_labels, svc_mdl.predict(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "param_grid = {\n",
    "            'penalty': ['l2'],\n",
    "            'class_weight': ['balanced'],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'sag'],\n",
    "            'multi_class': ['ovr', 'multinomial']\n",
    "            }\n",
    "\n",
    "\n",
    "#logr_grid = SGDClassifier(loss='hinge', penalty='l2', alpha=0.001, shuffle=True, verbose=0, \n",
    "#                    n_jobs=4, random_state=None, learning_rate='optimal')\n",
    "\n",
    "logr_grid = GridSearchCV(LogisticRegression(n_jobs=-1), param_grid)\n",
    "logr_mdl = logr_grid.fit(train_CustGray_2d, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Training score is', logr_mdl.score(train_data, train_labels))\n",
    "print('Validation score is', logr_mdl.score(test_data, test_labels))\n",
    "print('-'*80)\n",
    "print('Classification report of training data:\\n', classification_report(train_labels, logr_mdl.predict(train_data)))\n",
    "print('-'*80)\n",
    "print('Classification report of validation data:\\n', classification_report(test_labels, logr_mdl.predict(test_data)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
