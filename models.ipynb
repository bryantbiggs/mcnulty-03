{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pickled Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (7326,)\n",
      "Testing labels shape:  (2603,)\n"
     ]
    }
   ],
   "source": [
    "directory = 'generated_data'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "train_labels = np.load('{0}/train_labels.dat'.format(directory))\n",
    "test_labels = np.load('{0}/test_labels.dat'.format(directory))\n",
    "print('Training labels shape: {0}'.format(train_labels.shape))\n",
    "print('Testing labels shape:  {0}'.format(test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Custom Conversion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Conversion training shape: (7326, 640)\n",
      "Custom Conversion testing shape:  (2603, 640)\n"
     ]
    }
   ],
   "source": [
    "train_CustGray_2d = np.load('{0}/train_CustGray_2d.dat'.format(directory))\n",
    "test_CustGray_2d = np.load('{0}/test_CustGray_2d.dat'.format(directory))\n",
    "print('Custom Conversion training shape: {0}'.format(train_CustGray_2d.shape))\n",
    "print('Custom Conversion testing shape:  {0}'.format(test_CustGray_2d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Otsu's Binarization Threshold Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otsu training shape: (7326, 640)\n",
      "Otsu testing shape:  (2603, 640)\n"
     ]
    }
   ],
   "source": [
    "train_OBT_2d = np.load('{0}/train_OBT_2d.dat'.format(directory))\n",
    "test_OBT_2d = np.load('{0}/test_OBT_2d.dat'.format(directory))\n",
    "print('Otsu training shape: {0}'.format(train_OBT_2d.shape))\n",
    "print('Otsu testing shape:  {0}'.format(test_OBT_2d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Adaptive Mean Threshold Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive mean training shape: (7326, 640)\n",
      "Adaptive mean testing shape:  (2603, 640)\n"
     ]
    }
   ],
   "source": [
    "train_AMT_2d = np.load('{0}/train_AMT_2d.dat'.format(directory))\n",
    "test_AMT_2d = np.load('{0}/test_AMT_2d.dat'.format(directory))\n",
    "print('Adaptive mean training shape: {0}'.format(train_AMT_2d.shape))\n",
    "print('Adaptive mean testing shape:  {0}'.format(test_AMT_2d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Adaptive Gaussian Threshold Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive Gaussian training shape: (7326, 640)\n",
      "Adaptive Gaussian testing shape:  (2603, 640)\n"
     ]
    }
   ],
   "source": [
    "train_AGT_2d = np.load('{0}/train_AGT_2d.dat'.format(directory))\n",
    "test_AGT_2d = np.load('{0}/test_AGT_2d.dat'.format(directory))\n",
    "print('Adaptive Gaussian training shape: {0}'.format(train_AGT_2d.shape))\n",
    "print('Adaptive Gaussian testing shape:  {0}'.format(test_AGT_2d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Principle Component Analysis Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA training shape: (7326, 40)\n",
      "PCA testing shape:  (2603, 40)\n"
     ]
    }
   ],
   "source": [
    "train_PCA_2d = np.load('{0}/train_PCA_2d.dat'.format(directory))\n",
    "test_PCA_2d = np.load('{0}/test_PCA_2d.dat'.format(directory))\n",
    "print('PCA training shape: {0}'.format(train_PCA_2d.shape))\n",
    "print('PCA testing shape:  {0}'.format(test_PCA_2d.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lists of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ['Cust', 'OBT', 'AMT', 'AGT', 'PCA']\n",
    "l_train = [train_CustGray_2d, train_OBT_2d, train_AMT_2d, train_AGT_2d, train_PCA_2d]\n",
    "l_test = [test_CustGray_2d, test_OBT_2d, test_AMT_2d, test_AGT_2d, test_PCA_2d]\n",
    "overall_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Important Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_dir = 'imgs'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)\n",
    "    \n",
    "def important_pixels(mdl, save_img, ht=32, wd=20):\n",
    "    '''\n",
    "    source: http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html#example-ensemble-plot-forest-importances-faces-py\n",
    "    '''\n",
    "    importances = mdl.feature_importances_\n",
    "    try:\n",
    "        importances = importances.reshape(ht,wd)\n",
    "\n",
    "        # Plot pixel importances\n",
    "        plt.matshow(importances, cmap=plt.cm.hot)\n",
    "        plt.axis('off')\n",
    "        plt.savefig('{0}/{1}'.format(img_dir, save_img), bbox_inches='tight')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Attributes\n",
    "# estimators_ : list of classifiers\n",
    "# classes_ : array of shape = [n_classes]\n",
    "# n_classes_ : int\n",
    "# estimator_weights_ : array of floats\n",
    "# estimator_errors_ : array of floats\n",
    "# feature_importances_ : array of shape = [n_features]\n",
    "\n",
    "ab_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    n_estimators=50\n",
    "    \n",
    "    ab_clf = AdaBoostClassifier(base_estimator=None, n_estimators=n_estimators, learning_rate=1.0, \n",
    "                                 algorithm='SAMME.R', random_state=None)\n",
    "    \n",
    "    ab_mdl = ab_clf.fit(dataset, train_labels) \n",
    "   \n",
    "    scores = cross_val_score(ab_clf, dataset, train_labels)\n",
    "    \n",
    "    ab_dict[('ada_boost_{0}'.format(names[i]),n_estimators)] = scores.mean()*100\n",
    "    \n",
    "    _ = important_pixels(ab_mdl, 'ada_boost_{0}'.format(names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_dict['ada_boost'] = ab_dict\n",
    "ab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Attributes\n",
    "# base_estimator_ : list of estimators\n",
    "# estimators_ : list of estimators\n",
    "# estimators_samples_ : list of arrays\n",
    "# estimators_features_ : list of arrays\n",
    "# classes_ : array of shape = [n_classes]\n",
    "# n_classes_ : int or list\n",
    "# oob_score_ : float\n",
    "# oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
    "\n",
    "bag_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    bag_clf = BaggingClassifier(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, \n",
    "                                bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, \n",
    "                                n_jobs=-1, random_state=None, verbose=0)\n",
    "    scores = cross_val_score(bag_clf, dataset, train_labels)\n",
    "    bag_dict['bagging_{0}'.format(names[i])] = scores.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_dict['bagging'] = bag_dict\n",
    "bag_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier - TAKES A LONG TIME TO RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Attributes\n",
    "# feature_importances_ : array, shape = [n_features]\n",
    "# oob_improvement_ : array, shape = [n_estimators]\n",
    "# train_score_ : array, shape = [n_estimators]\n",
    "# loss_ : LossFunction\n",
    "# init : BaseEstimator\n",
    "# estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, loss_.K]\n",
    "    \n",
    "gb_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    n_estimators=100\n",
    "    gb_clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=n_estimators, subsample=1.0, \n",
    "                                     min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                     max_depth=3, init=None, random_state=None, max_features=None, verbose=0, \n",
    "                                     max_leaf_nodes=None, warm_start=False, presort='auto')\n",
    "    \n",
    "    gb_mdl = gb_clf.fit(dataset, train_labels)\n",
    "    scores = cross_val_score(gb_clf, dataset, train_labels)\n",
    "    \n",
    "    gb_dict[('gradient_boost_{0}'.format(names[i]),n_estimators)] = scores.mean()*100\n",
    "    \n",
    "    important_pixels(ab_mdl, 'gradient_boost_{0}'.format(names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_dict['gradient_boost'] = gb_dict\n",
    "gb_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# classes_ : array of shape = [n_classes] or a list of such arrays\n",
    "# feature_importances_ : array of shape = [n_features]\n",
    "# max_features_ : int,\n",
    "# n_classes_ : int or list\n",
    "# n_features_ : int\n",
    "# n_outputs_ : int\n",
    "# tree_ : Tree object\n",
    "\n",
    "dtr_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    dtr_clf = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, \n",
    "                            min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, \n",
    "                            max_leaf_nodes=None, class_weight=None, presort=False)\n",
    "\n",
    "    dtr_mdl = dtr_clf.fit(dataset, train_labels)\n",
    "    \n",
    "    scores = cross_val_score(dtr_clf, dataset, train_labels)\n",
    "    \n",
    "    dtr_dict['decision_tree_{0}'.format(names[i])] = scores.mean()*100\n",
    "    \n",
    "    important_pixels(dtr_mdl, 'decision_tree{0}'.format(names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_dict['decision_tree'] = dtr_dict\n",
    "dtr_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etr_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    \n",
    "    etr_clf = ExtraTreesClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                    min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, \n",
    "                    bootstrap=False, oob_score=False, n_jobs=-1, random_state=None, verbose=0, warm_start=False, \n",
    "                    class_weight=None)\n",
    "\n",
    "    etr_mdl = etr_clf.fit(dataset, train_labels)\n",
    "    \n",
    "    scores = cross_val_score(etr_clf, dataset, train_labels)\n",
    "    \n",
    "    etr_dict['extra_trees_{0}'.format(names[i])] = scores.mean()*100\n",
    "    \n",
    "    important_pixels(etr_mdl, 'extra_tree{0}'.format(names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_dict['extra_trees'] = etr_dict\n",
    "etr_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Attributes\n",
    "# estimators_ : list of DecisionTreeClassifier\n",
    "# classes_ : array of shape = [n_classes] or a list of such arrays\n",
    "# n_classes_ : int or list\n",
    "# n_features_ : int\n",
    "# n_outputs_ : int\n",
    "# feature_importances_ : array of shape = [n_features]\n",
    "# oob_score_ : float\n",
    "# oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
    "    \n",
    "rf_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    rf_clf = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                                min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "                                max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=-1, random_state=None, \n",
    "                                verbose=0, warm_start=False, class_weight=None)\n",
    "    \n",
    "    rf_mdl = rf_clf.fit(dataset, train_labels)\n",
    "    \n",
    "    scores = cross_val_score(rf_clf, dataset, train_labels)\n",
    "    \n",
    "    rf_dict['random_forrest_{0}'.format(names[i])] = scores.mean()*100\n",
    "    \n",
    "    important_pixels(rf_mdl, 'random_forrest{0}'.format(names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_dict['random_forrest'] = rf_dict\n",
    "rf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "knn_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                                   metric='minkowski', metric_params=None, n_jobs=-1)\n",
    "    \n",
    "    knn_mdl = knn_clf.fit(dataset, train_labels)\n",
    "    \n",
    "    scores = cross_val_score(knn_clf, dataset, train_labels)\n",
    "    \n",
    "    knn_dict['kneighbors_{0}'.format(names[i])] = scores.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_dict['kneighbors'] = knn_dict\n",
    "knn_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    svc_clf = SVC(C=1.0, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, \n",
    "                  tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, \n",
    "                  decision_function_shape=None)\n",
    "    \n",
    "    scores = cross_val_score(svc_clf, dataset, train_labels)\n",
    "    svc_dict['svc_{0}'.format(names[i])] = scores.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_dict['svc'] = svc_dict\n",
    "svc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "logr_dict = {}\n",
    "\n",
    "for i,dataset in enumerate(l_train):\n",
    "    logr_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, n_iter=5, \n",
    "                             shuffle=True, verbose=0, epsilon=0.1, n_jobs=-1, random_state=None, learning_rate='optimal', \n",
    "                             eta0=0.0, power_t=0.5, class_weight=None, warm_start=False, average=False)\n",
    "    \n",
    "    scores = cross_val_score(logr_clf, dataset, train_labels)\n",
    "    logr_dict['log_regression_{0}'.format(names[i])] = scores.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overall_dict['log_regression'] = logr_dict\n",
    "logr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(overall_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdls = [k for k,v in overall_dict.items()]\n",
    "lst_cnts = []\n",
    "for k,v in overall_dict.items():\n",
    "    temp = []\n",
    "    for x,y in overall_dict[v].values():\n",
    "        temp.append(y)\n",
    "    lst_cnts.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = range(1,6)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, menMeans, width, color='r', yerr=menStd)\n",
    "\n",
    "\n",
    "rects2 = ax.bar(ind + width, womenMeans, width, color='y', yerr=womenStd)\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Scores by group and gender')\n",
    "ax.set_xticks(ind + width)\n",
    "ax.set_xticklabels(('G1', 'G2', 'G3', 'G4', 'G5'))\n",
    "\n",
    "ax.legend((rects1[0], rects2[0]), ('Men', 'Women'))\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    # attach some text labels\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                '%d' % int(height),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make figure\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plots\n",
    "plt.bar(range(len(train_cnt_dict)), train_cnt_dict.values(), , color=colors[5], \n",
    "                alpha=0.5, edgecolor='w', label='Training Labels')\n",
    "\n",
    "# labels/titles\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Distribution of Class Labels')\n",
    "\n",
    "plt.xlim(-0.5,9.5)\n",
    "ticks = [0,1,2,3,4,5,6,7,8,9]\n",
    "plt.xticks(ticks, ticks)\n",
    "\n",
    "# remove border\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False) \n",
    "ax.spines[\"bottom\"].set_visible(False) \n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "# show grid\n",
    "ax.yaxis.grid(True) \n",
    "\n",
    "# plot that biddy\n",
    "plt.tight_layout()\n",
    "_ = plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
